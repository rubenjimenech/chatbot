{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubenjimenech/chatbot/blob/main/ChatbotRubenJimenezipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ],
      "metadata": {
        "id": "nmYpycZv7oyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# load data from JSON file\n",
        "with open('intents.json') as file:  # Reemplaza '/content/intents.json' con la ruta correcta a tu archivo JSON\n",
        "    data = json.load(file)\n",
        "\n",
        "# extract text and intent from data\n",
        "texts = []\n",
        "intents = []\n",
        "for intent in data['intents']:\n",
        "    for text in intent['patterns']:\n",
        "        texts.append(text)\n",
        "        intents.append(intent['tag'])\n",
        "\n",
        "# tokenize text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "encoded_texts = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# save tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# pad sequences to have equal length\n",
        "max_len = max([len(x) for x in encoded_texts])\n",
        "padded_texts = pad_sequences(encoded_texts, maxlen=max_len, padding='post')\n",
        "\n",
        "# create label encoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# fit and transform the intents to integer labels\n",
        "encoded_intents = le.fit_transform(intents)\n",
        "\n",
        "# get the number of unique labels\n",
        "num_intents = len(le.classes_)\n",
        "\n",
        "# apply one-hot encoding to the integer labels\n",
        "encoded_intents = tf.one_hot(encoded_intents, depth=num_intents)\n",
        "\n",
        "# define model architecture\n",
        "input_layer = Input(shape=(max_len,))\n",
        "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_len)(input_layer)\n",
        "lstm_layer = LSTM(128)(embedding_layer)\n",
        "output_layer = Dense(num_intents, activation='softmax')(lstm_layer)\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# train model\n",
        "model.fit(padded_texts, encoded_intents, epochs=25, batch_size=16)\n",
        "\n",
        "# save model\n",
        "model.save('chatbot_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOFhFybS7yF9",
        "outputId": "8a597eaf-4999-4e07-de24-e99c079ffce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "12/12 [==============================] - 5s 15ms/step - loss: 3.9529 - accuracy: 0.0479\n",
            "Epoch 2/25\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 3.9242 - accuracy: 0.0691\n",
            "Epoch 3/25\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 3.8591 - accuracy: 0.0691\n",
            "Epoch 4/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 3.7811 - accuracy: 0.0851\n",
            "Epoch 5/25\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 3.6758 - accuracy: 0.1117\n",
            "Epoch 6/25\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 3.5512 - accuracy: 0.0851\n",
            "Epoch 7/25\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 3.4862 - accuracy: 0.0904\n",
            "Epoch 8/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 3.4370 - accuracy: 0.0851\n",
            "Epoch 9/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 3.3489 - accuracy: 0.0904\n",
            "Epoch 10/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 3.2550 - accuracy: 0.0957\n",
            "Epoch 11/25\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 3.1411 - accuracy: 0.1223\n",
            "Epoch 12/25\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 2.9923 - accuracy: 0.1596\n",
            "Epoch 13/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.7818 - accuracy: 0.1968\n",
            "Epoch 14/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.6075 - accuracy: 0.2553\n",
            "Epoch 15/25\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 2.3525 - accuracy: 0.2979\n",
            "Epoch 16/25\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 2.1486 - accuracy: 0.3404\n",
            "Epoch 17/25\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.9353 - accuracy: 0.3723\n",
            "Epoch 18/25\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.7597 - accuracy: 0.4894\n",
            "Epoch 19/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1.5842 - accuracy: 0.6011\n",
            "Epoch 20/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1.3555 - accuracy: 0.6436\n",
            "Epoch 21/25\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.2179 - accuracy: 0.7394\n",
            "Epoch 22/25\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.1038 - accuracy: 0.7660\n",
            "Epoch 23/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.8875 - accuracy: 0.8564\n",
            "Epoch 24/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.7185 - accuracy: 0.9415\n",
            "Epoch 25/25\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.5918 - accuracy: 0.9202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JwJyk5l92WxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load data from JSON file\n",
        "with open('/content/intents.json') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Extract text patterns and intents from data\n",
        "patterns = []\n",
        "intents = []\n",
        "for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        patterns.append(pattern)\n",
        "        intents.append(intent['tag'])\n",
        "\n",
        "# Tokenize text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(patterns)\n",
        "\n",
        "# Load saved model\n",
        "model = load_model('/content/chatbot_model.h5')\n",
        "\n",
        "# Define maximum sequence length\n",
        "max_len = model.input_shape[1]\n",
        "\n",
        "# Create label encoder object\n",
        "le = LabelEncoder()\n",
        "le.fit(intents)\n",
        "\n",
        "# Create inverse mapping of label encoder for intent prediction\n",
        "intent_mapping = {i: label for i, label in enumerate(le.classes_)}\n",
        "\n",
        "# Example usage:\n",
        "# Suppose you have a sequence of text you want to classify:\n",
        "input_text = \"Recommend a history book\"\n",
        "\n",
        "# Tokenize and pad the input text\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])\n",
        "padded_text = pad_sequences(encoded_text, maxlen=max_len, padding='post')\n",
        "\n",
        "# Predict intent\n",
        "predicted = model.predict(padded_text)\n",
        "predicted_label = intent_mapping[np.argmax(predicted)]\n",
        "\n",
        "print(f\"Predicted Intent: {predicted_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSJ9oHKF9wcK",
        "outputId": "a6a12da9-3b19-42d9-84fd-c658960d1c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 455ms/step\n",
            "Predicted Intent: History\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mensajes_despedida=['adios','bye','hasta luego','good bye','stop','quit']\n",
        "etiquetas_basicas=[\"greeting\",\"goodbye\",\"thanks\",\"book_search\"]"
      ],
      "metadata": {
        "id": "fuy4QBLjNB6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('Welcome to the chatbot! Type \"quit\" to exit.')\n",
        "while True:\n",
        "    # get user input\n",
        "    user_input = input('You: ').lower().strip()  #for user input run this line\n",
        "    # user_input = \"quit\"\n",
        "\n",
        "    # check if user wants to quit\n",
        "    if user_input in mensajes_despedida:\n",
        "        break\n",
        "\n",
        "# encode user input text\n",
        "    encoded_input = tokenizer.texts_to_sequences([user_input])\n",
        "    padded_input = pad_sequences(encoded_input, maxlen=max_len, padding='post')\n",
        "\n",
        "    # predict intent\n",
        "    intent_prob = model.predict(padded_input)[0]\n",
        "    intent_idx = np.argmax(intent_prob)\n",
        "    intent_label = le.inverse_transform([intent_idx])[0]\n",
        "\n",
        "    # retrieve response\n",
        "    for intent in data['intents']:\n",
        "        if intent['tag'] == intent_label:\n",
        "          response = np.random.choice(intent['responses'])\n",
        "          if intent_label in etiquetas_basicas:\n",
        "            print('Chatbot:', response)\n",
        "          else:\n",
        "            print('Chatbot:', response['Book'],'. RATE: ', response['Rate'])\n",
        "\n",
        "            print('Chatbot: Do you want a description of this book?')\n",
        "            entrada = int(input('You:   1: YES,  if you dont want, press any key   '))\n",
        "\n",
        "            if entrada==1:\n",
        "              print('Chatbot:', response['Feedback'])\n",
        "\n",
        "              print('\\nChatbot: Do you have any question?')\n",
        "            else:\n",
        "              print('Chatbot: Do you have any question?')\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXjt_mlc-3_G",
        "outputId": "4aefb649-a5bb-4a9a-e44e-d1fbb693753a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the chatbot! Type \"quit\" to exit.\n",
            "You: hello\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Chatbot: Hey! What brings you here today?\n",
            "You: hola\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Chatbot: Hi there! What can I do for you?\n",
            "You: hi \n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Chatbot: Hey! What brings you here today?\n",
            "You: recommend a history book\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "Chatbot: Presidential Power and the Modern Presidents . RATE:  3.79\n",
            "Chatbot: Do you want a description of this book?\n",
            "You:   1: YES,  if you dont want, press any key   1\n",
            "Chatbot: Suggests a theory of presidential power, and tests it against the events in the administrations of the postwar presidents\n",
            "\n",
            "Chatbot: Do you have any question?\n",
            "You: What is the book with the best  rating?\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Chatbot: Men Are from Mars, Women Are from Venus . RATE:  3.54\n",
            "Chatbot: Do you want a description of this book?\n",
            "You:   1: YES,  if you dont want, press any key   2\n",
            "Chatbot: Do you have any question?\n",
            "You: What is the book with the highest rating?\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Chatbot: The Black Veil . RATE:  3.06\n",
            "Chatbot: Do you want a description of this book?\n",
            "You:   1: YES,  if you dont want, press any key   1\n",
            "Chatbot: The author weaves together past and present and family legend as he shares his personal story of dealing with depression, and his search through his family's paternal lines to find clues to his melancholy.\n",
            "\n",
            "Chatbot: Do you have any question?\n",
            "You: bye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.choice(intent['responses'])\n"
      ],
      "metadata": {
        "id": "Itu6H6kKq4L2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a62f03-b94d-4535-eed1-0d4af0aac655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Book': 'The Black Veil',\n",
              " 'Feedback': \"The author weaves together past and present and family legend as he shares his personal story of dealing with depression, and his search through his family's paternal lines to find clues to his melancholy.\",\n",
              " 'Rate': 3.06}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response['Book']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3wWEgzVwS2Ki",
        "outputId": "6b3760a5-6f76-4f6b-9427-bb5e67fe29ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Black Veil'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response['Feedback']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "9HdxvKgC40yW",
        "outputId": "4c12f0db-411f-493d-92a2-2b61301b6915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The author weaves together past and present and family legend as he shares his personal story of dealing with depression, and his search through his family's paternal lines to find clues to his melancholy.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    }
  ]
}