{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubenjimenech/chatbot/blob/main/ChatbotRubenJimenezipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ],
      "metadata": {
        "id": "nmYpycZv7oyV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# load data from JSON file\n",
        "with open('intents.json') as file:  # Reemplaza '/content/intents.json' con la ruta correcta a tu archivo JSON\n",
        "    data = json.load(file)\n",
        "\n",
        "# extract text and intent from data\n",
        "texts = []\n",
        "intents = []\n",
        "for intent in data['intents']:\n",
        "    for text in intent['patterns']:\n",
        "        texts.append(text)\n",
        "        intents.append(intent['tag'])\n",
        "\n",
        "# tokenize text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "encoded_texts = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# save tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# pad sequences to have equal length\n",
        "max_len = max([len(x) for x in encoded_texts])\n",
        "padded_texts = pad_sequences(encoded_texts, maxlen=max_len, padding='post')\n",
        "\n",
        "# create label encoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# fit and transform the intents to integer labels\n",
        "encoded_intents = le.fit_transform(intents)\n",
        "\n",
        "# get the number of unique labels\n",
        "num_intents = len(le.classes_)\n",
        "\n",
        "# apply one-hot encoding to the integer labels\n",
        "encoded_intents = tf.one_hot(encoded_intents, depth=num_intents)\n",
        "\n",
        "# define model architecture\n",
        "input_layer = Input(shape=(max_len,))\n",
        "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_len)(input_layer)\n",
        "lstm_layer = LSTM(128)(embedding_layer)\n",
        "output_layer = Dense(num_intents, activation='softmax')(lstm_layer)\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# train model\n",
        "model.fit(padded_texts, encoded_intents, epochs=25, batch_size=16)\n",
        "\n",
        "# save model\n",
        "model.save('chatbot_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOFhFybS7yF9",
        "outputId": "92c8c815-15da-4892-d4af-513ff166a733"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "12/12 [==============================] - 3s 24ms/step - loss: 3.9524 - accuracy: 0.0691\n",
            "Epoch 2/25\n",
            "12/12 [==============================] - 0s 40ms/step - loss: 3.9265 - accuracy: 0.0691\n",
            "Epoch 3/25\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 3.8671 - accuracy: 0.0691\n",
            "Epoch 4/25\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 3.7890 - accuracy: 0.0691\n",
            "Epoch 5/25\n",
            "12/12 [==============================] - 0s 41ms/step - loss: 3.7072 - accuracy: 0.0798\n",
            "Epoch 6/25\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 3.5901 - accuracy: 0.0904\n",
            "Epoch 7/25\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 3.5075 - accuracy: 0.0957\n",
            "Epoch 8/25\n",
            "12/12 [==============================] - 0s 38ms/step - loss: 3.4166 - accuracy: 0.0851\n",
            "Epoch 9/25\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 3.3080 - accuracy: 0.0851\n",
            "Epoch 10/25\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 3.1764 - accuracy: 0.0957\n",
            "Epoch 11/25\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 3.0274 - accuracy: 0.1330\n",
            "Epoch 12/25\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 2.8304 - accuracy: 0.1968\n",
            "Epoch 13/25\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 2.5902 - accuracy: 0.2553\n",
            "Epoch 14/25\n",
            "12/12 [==============================] - 1s 39ms/step - loss: 2.3811 - accuracy: 0.2872\n",
            "Epoch 15/25\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 2.1990 - accuracy: 0.3138\n",
            "Epoch 16/25\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.9090 - accuracy: 0.4787\n",
            "Epoch 17/25\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 1.6944 - accuracy: 0.5213\n",
            "Epoch 18/25\n",
            "12/12 [==============================] - 1s 44ms/step - loss: 1.4804 - accuracy: 0.6277\n",
            "Epoch 19/25\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 1.2930 - accuracy: 0.6915\n",
            "Epoch 20/25\n",
            "12/12 [==============================] - 0s 39ms/step - loss: 1.0715 - accuracy: 0.8138\n",
            "Epoch 21/25\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8941 - accuracy: 0.8723\n",
            "Epoch 22/25\n",
            "12/12 [==============================] - 0s 38ms/step - loss: 0.7399 - accuracy: 0.9309\n",
            "Epoch 23/25\n",
            "12/12 [==============================] - 0s 40ms/step - loss: 0.6445 - accuracy: 0.9043\n",
            "Epoch 24/25\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.5362 - accuracy: 0.9574\n",
            "Epoch 25/25\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 0.4378 - accuracy: 0.9840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load data from JSON file\n",
        "with open('/content/intents.json') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Extract text patterns and intents from data\n",
        "patterns = []\n",
        "intents = []\n",
        "for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        patterns.append(pattern)\n",
        "        intents.append(intent['tag'])\n",
        "\n",
        "# Tokenize text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(patterns)\n",
        "\n",
        "# Load saved model\n",
        "model = load_model('/content/chatbot_model.h5')\n",
        "\n",
        "# Define maximum sequence length\n",
        "max_len = model.input_shape[1]\n",
        "\n",
        "# Create label encoder object\n",
        "le = LabelEncoder()\n",
        "le.fit(intents)\n",
        "\n",
        "# Create inverse mapping of label encoder for intent prediction\n",
        "intent_mapping = {i: label for i, label in enumerate(le.classes_)}\n",
        "\n",
        "# Example usage:\n",
        "# Suppose you have a sequence of text you want to classify:\n",
        "input_text = \"Recommend a history book\"\n",
        "\n",
        "# Tokenize and pad the input text\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])\n",
        "padded_text = pad_sequences(encoded_text, maxlen=max_len, padding='post')\n",
        "\n",
        "# Predict intent\n",
        "predicted = model.predict(padded_text)\n",
        "predicted_label = intent_mapping[np.argmax(predicted)]\n",
        "\n",
        "print(f\"Predicted Intent: {predicted_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSJ9oHKF9wcK",
        "outputId": "e1aafdbd-8dd7-4374-c5b8-e71059235522"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 559ms/step\n",
            "Predicted Intent: History\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mensajes_despedida=['adios','bye','hasta luego','good bye','stop','quit']\n",
        "etiquetas_basicas=[\"greeting\",\"goodbye\",\"thanks\",\"book_search\"]"
      ],
      "metadata": {
        "id": "fuy4QBLjNB6v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('Welcome to the chatbot! Type \"quit\" to exit.')\n",
        "while True:\n",
        "    # get user input\n",
        "    user_input = input('You: ').lower().strip()  #for user input run this line\n",
        "    # user_input = \"quit\"\n",
        "\n",
        "    # check if user wants to quit\n",
        "    if user_input in mensajes_despedida:\n",
        "        break\n",
        "\n",
        "# encode user input text\n",
        "    encoded_input = tokenizer.texts_to_sequences([user_input])\n",
        "    padded_input = pad_sequences(encoded_input, maxlen=max_len, padding='post')\n",
        "\n",
        "    # predict intent\n",
        "    intent_prob = model.predict(padded_input)[0]\n",
        "    intent_idx = np.argmax(intent_prob)\n",
        "    intent_label = le.inverse_transform([intent_idx])[0]\n",
        "\n",
        "    # retrieve response\n",
        "    for intent in data['intents']:\n",
        "        if intent['tag'] == intent_label:\n",
        "          response = np.random.choice(intent['responses'])\n",
        "          if intent_label in etiquetas_basicas:\n",
        "            print('Chatbot:', response)\n",
        "          else:\n",
        "            print('Chatbot:', response['Book'],'. RATE: ', response['Rate'])\n",
        "\n",
        "            print('Chatbot: Do you want a description of this book?')\n",
        "            entrada = int(input('You:   1: YES,  if you dont want, press any key   '))\n",
        "\n",
        "            if entrada==1:\n",
        "              print('Chatbot:', response['Feedback'])\n",
        "\n",
        "              print('\\nChatbot: Do you have any question?')\n",
        "            else:\n",
        "              print('Chatbot: Do you have any question?')\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXjt_mlc-3_G",
        "outputId": "c5e297d1-9bf2-4183-b587-80a130a507ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the chatbot! Type \"quit\" to exit.\n",
            "You: Hello\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Chatbot: Hey! What brings you here today?\n",
            "You: I'm looking for a book\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Chatbot: Certainly! What genre are you interested in?\n",
            "You: Recommend a history book\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Chatbot: A Crack in the Edge of the World . RATE:  3.78\n",
            "Chatbot: Do you want a description of this book?\n",
            "You:   1: YES,  if you dont want, press any key   1\n",
            "Chatbot: Unleashed by ancient geologic forces, a magnitude 8.25 earthquake rocked San Francisco in the early hours of April 18, 1906. Less than a minute later, the city lay in ruins. Bestselling author Simon Winchester brings his inimitable storytelling abilities to this extraordinary event, exploring the legendary earthquake and fires that spread horror across San Francisco and northern California in 1906 as well as its startling impact on American history and, just as important, what science has recently revealed about the fascinating subterranean processes that produced it—and almost certainly will cause it to strike again.\n",
            "\n",
            "Chatbot: Do you have any question?\n",
            "You: bye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.choice(intent['responses'])\n"
      ],
      "metadata": {
        "id": "Itu6H6kKq4L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response['Book']"
      ],
      "metadata": {
        "id": "3wWEgzVwS2Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response['Feedback']"
      ],
      "metadata": {
        "id": "9HdxvKgC40yW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}